import{RLRealRange}from"../rl/base.js";import NeuralNetwork from"./neuralnetwork.js";class ActorCriticNet{constructor(t,e=20,s=[],a="sgd"){this._resolution=e,this._states=t.states,this._actions=t.actions,this._action_sizes=t.actions.map((t=>Array.isArray(t)?t.length:e)),this._layers=[{type:"input",name:"state"},...s,{type:"linear",name:"d"},{type:"full",out_size:1,name:"value"},{type:"full",out_size:this._action_sizes.reduce(((t,e)=>t*e),1),input:["d"],name:"actor"},{type:"softmax",name:"prob"},{type:"log",name:"log_prob"},{type:"input",name:"action"},{type:"mult",input:["log_prob","action"]},{type:"sum",axis:1,name:"action_log_prob"},{type:"mult",input:["log_prob","prob"]},{type:"sum",axis:1},{type:"mean"},{type:"negative",name:"entropy"},{type:"input",name:"reward"},{type:"sub",input:["reward","value"],name:"advantages"},{type:"power",n:2},{type:"mean",name:"value_loss"},{type:"detach",input:["advantages"],name:"detach_adv"},{type:"mult",input:["action_log_prob","detach_adv"]},{type:"mean",name:"action_gain"},{type:"mult",input:[.5,"value_loss"],name:"value_c"},{type:"mult",input:[.01,"entropy"],name:"entropy_c"},{type:"sub",input:["value_c","action_gain","entropy_c"]},{type:"mean"}],this._net=NeuralNetwork.fromObject(this._layers,null,a)}get_action(t){t=this._state_to_input(t);const e=this._net.calc([t],null,["prob"]).prob.toArray()[0];let s=Math.random();for(let t=0;t<e.length;t++)if(s-=e[t],s<0)return[t];return[0]}_state_to_input(t){const e=[];for(let s=0;s<t.length;s++)if(Array.isArray(this._states[s]))for(let a=0;a<this._states[s].length;a++)e.push(this._states[s][a]===t[s]?1:0);else e.push(t[s]);return e}get_score(t){if(t)return this._net.calc(t.map((t=>this._state_to_input(t))),null,["value"]).value.toArray();if(!this._states_data){const t=this._states.map((t=>t.toArray(this._resolution).length));this._states_data=[];const e=e=>{for(let s=0;s<e.length;s++){if(e[s]++,e[s]<t[s])return!0;e[s]=0}return!1},s=Array(this._states.length).fill(0);do{this._states_data.push([].concat(s))}while(e(s))}const e=this._net.calc(this._states_data,null,["prob"]).prob.toArray(),s=[],a=this._states.length;for(let t=0;t<this._states_data.length;t++){let i=s;for(let e=0;e<a-1;e++)i[this._states_data[t][e]]||(i[this._states_data[t][e]]=[]),i=i[this._states_data[t][e]];i[this._states_data[t][a-1]]=e[t]}return s}_action_pos(t){let e=0;for(let s=0;s<t.length;s++)if(e*=this._action_sizes[s],Array.isArray(this._actions[s]))e+=this._actions[s].indexOf(t[s]);else{if(!(this._actions[s]instanceof RLRealRange))throw"Not implemented";e+=this._actions[s].indexOf(t[s],this._resolution)}return e}update(t,e,s,a,i){t=t.map((t=>this._state_to_input(t))),e=e.map((t=>{const e=this._action_pos(t),s=Array(this._action_sizes.reduce(((t,e)=>t*e),1)).fill(0);return s[e]=1,s})),this._net.fit({state:t,action:e,reward:s},null,1,a,i)}}export default class A2CAgent{constructor(t,e,s,a,i){this._net=new ActorCriticNet(t,e,a,i),this._procs=s,this._env=t,this._advanced_step=5,this._gamma=.99,this._init_states=[];for(let t=0;t<1e3;t++)this._init_states.push(this._env.reset());this._states=[];for(let t=0;t<this._procs;t++)this._states[t]=this._env.reset()}terminate(){}get_score(){return this._net.get_score()}get_action(t){return this._net.get_action(t)}update(t,e,s){const a=[],i=[],n=[],_=[],o=[];for(let t=0;t<this._advanced_step;t++)for(let t=0;t<this._procs;t++){const e=this._net.get_action(this._states[t]),s=this._env.test(this._states[t],e);(a[t]||=[]).push(e),(i[t]||=[]).push(this._states[t]),(n[t]||=[]).push(s.state),(_[t]||=[]).push(s.reward),(o[t]||=[]).push(s.done),s.done&&(this._states[t]=this._init_states.pop()),s.done&&this._states[t]||(this._states[t]||console.warn("Due to implementation limitations, the environment of some Agents was not initialized."),this._states[t]=s.state)}const r=this._net.get_score(n.map((t=>t[t.length-1]))),h=[];for(let t=0;t<this._procs;t++)h[t]=[],h[t][this._advanced_step-1]=r[t][0];for(let t=this._advanced_step-2;t>=0;t--)for(let e=0;e<this._procs;e++)h[e][t]=_[e][t],o[e][t]||(h[e][t]+=h[e][t+1]*this._gamma);if(this._net.update(i.flat(1),a.flat(1),h.flat(1).map((t=>[t])),e,s),t)for(;this._init_states.length<1e3;)this._init_states.push(this._env.reset())}}