// This file is generated automatically.
export { default as AddLayer } from './add.js'
export { default as AdditiveCoupling } from './additive_coupling.js'
export { default as AndLayer } from './and.js'
export { default as AdaptivePiecewiseLinearLayer } from './apl.js'
export { default as ArandaLayer } from './aranda.js'
export { default as ArgmaxLayer } from './argmax.js'
export { default as AveragePoolLayer } from './averagepool.js'
export { default as BatchNormalizationLayer } from './batch_normalization.js'
export { default as BimodalDerivativeAdaptiveActivationLayer } from './bdaa.js'
export { default as BitwiseAndLayer } from './bitwise_and.js'
export { default as BitwiseOrLayer } from './bitwise_or.js'
export { default as BitwiseXorLayer } from './bitwise_xor.js'
export { default as BendableLinearUnitLayer } from './blu.js'
export { default as BoundedReLULayer } from './brelu.js'
export { default as ContinuouslyDifferentiableELULayer } from './celu.js'
export { default as ClipLayer } from './clip.js'
export { default as ConcatLayer } from './concat.js'
export { default as CondLayer } from './cond.js'
export { default as ConstLayer } from './const.js'
export { default as ConvLayer } from './conv.js'
export { default as ConcatenatedReLULayer } from './crelu.js'
export { default as DetachLayer } from './detach.js'
export { default as DivLayer } from './div.js'
export { default as DropoutLayer } from './dropout.js'
export { default as ElasticELULayer } from './eelu.js'
export { default as ELULayer } from './elu.js'
export { default as EqualLayer } from './equal.js'
export { default as ElasticReLULayer } from './erelu.js'
export { default as ESwishLayer } from './eswish.js'
export { default as FastELULayer } from './felu.js'
export { default as FlattenLayer } from './flatten.js'
export { default as FlexibleReLULayer } from './frelu.js'
export { default as FullyConnected } from './full.js'
export { default as GaussianLayer } from './gaussian.js'
export { default as GlobalAveragePoolLayer } from './global_averagepool.js'
export { default as GlobalLpPoolLayer } from './global_lppool.js'
export { default as GlobalMaxPoolLayer } from './global_maxpool.js'
export { default as GreaterLayer } from './greater.js'
export { default as GreaterOrEqualLayer } from './greater_or_equal.js'
export { default as GRULayer } from './gru.js'
export { default as HardShrinkLayer } from './hard_shrink.js'
export { default as HardSigmoidLayer } from './hard_sigmoid.js'
export { default as HardTanhLayer } from './hard_tanh.js'
export { default as HexpoLayer } from './hexpo.js'
export { default as HuberLayer } from './huber.js'
export { default as IdentityLayer } from './identity.js'
export { default as IncludeLayer } from './include.js'
export { default as InputLayer } from './input.js'
export { default as ImprovedSigmoidLayer } from './isigmoid.js'
export { default as LeakyReLULayer } from './leaky_relu.js'
export { default as LessLayer } from './less.js'
export { default as LessOrEqualLayer } from './less_or_equal.js'
export { default as LpPoolLayer } from './lppool.js'
export { default as LRNLayer } from './lrn.js'
export { default as LSTMLayer } from './lstm.js'
export { default as MatmulLayer } from './matmul.js'
export { default as MaxLayer } from './max.js'
export { default as MaxPoolLayer } from './maxpool.js'
export { default as MeanLayer } from './mean.js'
export { default as MinLayer } from './min.js'
export { default as MultipleParametricELULayer } from './mpelu.js'
export { default as MSELayer } from './mse.js'
export { default as MultibinTrainableLinearUnitLayer } from './mtlu.js'
export { default as MultLayer } from './mult.js'
export { default as NaturalLogarithmReLULayer } from './nlrelu.js'
export { default as OnehotLayer } from './onehot.js'
export { default as OrLayer } from './or.js'
export { default as OutputLayer } from './output.js'
export { default as PadeActivationUnitLayer } from './pau.js'
export { default as ParametricDeformableELULayer } from './pdelu.js'
export { default as ParametricELULayer } from './pelu.js'
export { default as PiecewiseLinearUnitLayer } from './plu.js'
export { default as PowerLayer } from './power.js'
export { default as ParametricReLULayer } from './prelu.js'
export { default as ParametricRectifiedExponentialUnitLayer } from './preu.js'
export { default as ParametricSigmoidFunctionLayer } from './psf.js'
export { default as PenalizedTanhLayer } from './ptanh.js'
export { default as ParametricTanhLinearUnitLayer } from './ptelu.js'
export { default as RandomLayer } from './random.js'
export { default as RectifiedPowerUnitLayer } from './repu.js'
export { default as ReshapeLayer } from './reshape.js'
export { default as ReverseLayer } from './reverse.js'
export { default as RNNLayer } from './rnn.js'
export { default as RandomizedReLULayer } from './rrelu.js'
export { default as RandomTranslationReLULayer } from './rtrelu.js'
export { default as ScaledELULayer } from './selu.js'
export { default as SigmoidLayer } from './sigmoid.js'
export { default as SelfLearnableAFLayer } from './slaf.js'
export { default as SoftplusLinearUnitLayer } from './slu.js'
export { default as SoftShrinkLayer } from './soft_shrink.js'
export { default as SoftargmaxLayer } from './softargmax.js'
export { default as SoftmaxLayer } from './softmax.js'
export { default as SoftplusLayer } from './softplus.js'
export { default as SparseLayer } from './sparse.js'
export { default as SplitLayer } from './split.js'
export { default as ShiftedReLULayer } from './srelu.js'
export { default as SoftRootSignLayer } from './srs.js'
export { default as ScaledTanhLayer } from './stanh.js'
export { default as StdLayer } from './std.js'
export { default as SubLayer } from './sub.js'
export { default as SumLayer } from './sum.js'
export { default as SupervisorLayer } from './supervisor.js'
export { default as SwishLayer } from './swish.js'
export { default as TrainableAFLayer } from './taf.js'
export { default as ThresholdedReLULayer } from './thresholded_relu.js'
export { default as TransposeLayer } from './transpose.js'
export { default as VariableLayer } from './variable.js'
export { default as VarLayer } from './variance.js'
export { default as XorLayer } from './xor.js'

import Matrix from '../../../util/matrix.js'
import Tensor from '../../../util/tensor.js'
/**
 * @typedef {(
 * { type: 'abs' } |
 * { type: 'acos' } |
 * { type: 'acosh' } |
 * { type: 'add' } |
 * { type: 'additive_coupling', d?: number | null, net?: NeuralNetwork | *[] | null } |
 * { type: 'and' } |
 * { type: 'apl', s?: number, a?: number | number[], b?: number | number[] } |
 * { type: 'aranda', l?: number } |
 * { type: 'argmax' } |
 * { type: 'asin' } |
 * { type: 'asinh' } |
 * { type: 'atan' } |
 * { type: 'atanh' } |
 * { type: 'average_pool', kernel: number | number[], stride?: number | number[], padding?: number | number[], channel_dim?: number } |
 * { type: 'batch_normalization', scale?: number, offset?: number } |
 * { type: 'bdaa', alpha?: number } |
 * { type: 'bent_identity' } |
 * { type: 'bitwise_and' } |
 * { type: 'bitwise_not' } |
 * { type: 'bitwise_or' } |
 * { type: 'bitwise_xor' } |
 * { type: 'blu', beta?: number } |
 * { type: 'brelu', a?: number } |
 * { type: 'ceil' } |
 * { type: 'celu', a?: number } |
 * { type: 'clip', min?: number, max?: number } |
 * { type: 'cloglog' } |
 * { type: 'cloglogm' } |
 * { type: 'concat', axis?: number } |
 * { type: 'cond' } |
 * { type: 'const', value: number } |
 * { type: 'conv', kernel: number | number[], channel?: number, stride?: number | number[], padding?: number | number[], w?: number[][] | Tensor | string, activation?: string, l2_decay?: number, l1_decay?: number, activation_params?: object, channel_dim?: number } |
 * { type: 'cos' } |
 * { type: 'cosh' } |
 * { type: 'crelu' } |
 * { type: 'detach' } |
 * { type: 'div' } |
 * { type: 'dropout', drop_rate?: number } |
 * { type: 'eelu', k?: number, alpha?: number, beta?: number } |
 * { type: 'elish' } |
 * { type: 'elliott' } |
 * { type: 'elu', a?: number } |
 * { type: 'equal' } |
 * { type: 'erelu' } |
 * { type: 'erf' } |
 * { type: 'eswish', beta?: number } |
 * { type: 'exp' } |
 * { type: 'felu', alpha?: number } |
 * { type: 'flatten' } |
 * { type: 'floor' } |
 * { type: 'frelu', b?: number } |
 * { type: 'full', out_size: number | string, w?: number[][] | Matrix | string, b?: number[][] | Matrix | string, activation?: string, l2_decay?: number, l1_decay?: number, activation_params?: object } |
 * { type: 'gaussian' } |
 * { type: 'geru' } |
 * { type: 'global_average_pool', channel_dim?: number } |
 * { type: 'global_lp_pool', p?: number, channel_dim?: number } |
 * { type: 'global_max_pool', channel_dim?: number } |
 * { type: 'greater' } |
 * { type: 'greater_or_equal' } |
 * { type: 'gru', size: number, return_sequences?: boolean, w_z?: number[][] | Matrix, w_r?: number[][] | Matrix, w_h?: number[][] | Matrix, u_z?: number[][] | Matrix, u_r?: number[][] | Matrix, u_h?: number[][] | Matrix, b_z?: number[][] | Matrix, b_r?: number[][] | Matrix, b_h?: number[][] | Matrix } |
 * { type: 'hard_elish' } |
 * { type: 'hard_shrink', l?: number } |
 * { type: 'hard_sigmoid', alpha?: number, beta?: number } |
 * { type: 'hard_swish' } |
 * { type: 'hard_tanh', v?: number } |
 * { type: 'hexpo', a?: number, b?: number, c?: number, d?: number } |
 * { type: 'huber' } |
 * { type: 'identity' } |
 * { type: 'include', net: NeuralNetwork | object[], input_to?: string, train?: boolean } |
 * { type: 'input', name?: string } |
 * { type: 'is_inf' } |
 * { type: 'is_nan' } |
 * { type: 'isigmoid', a?: number, alpha?: number } |
 * { type: 'leaky_relu', a?: number } |
 * { type: 'less' } |
 * { type: 'less_or_equal' } |
 * { type: 'lisht' } |
 * { type: 'log' } |
 * { type: 'loglog' } |
 * { type: 'logsigmoid' } |
 * { type: 'lp_pool', p?: number, kernel: number | number[], stride?: number | number[], padding?: number | number[], channel_dim?: number } |
 * { type: 'lrn', alpha?: number, beta?: number, k?: number, n: number, channel_dim?: number } |
 * { type: 'lstm', size: number, return_sequences?: boolean, w_z?: number[][] | Matrix, w_in?: number[][] | Matrix, w_for?: number[][] | Matrix, w_out?: number[][] | Matrix, r_z?: number[][] | Matrix, r_in?: number[][] | Matrix, r_for?: number[][] | Matrix, r_out?: number[][] | Matrix, p_in?: number[][] | Matrix, p_for?: number[][] | Matrix, p_out?: number[][] | Matrix, b_z?: number[][] | Matrix, b_in?: number[][] | Matrix, b_for?: number[][] | Matrix, b_out?: number[][] | Matrix } |
 * { type: 'matmul' } |
 * { type: 'max' } |
 * { type: 'max_pool', kernel: number | number[], stride?: number | number[], padding?: number | number[], channel_dim?: number } |
 * { type: 'mean', axis?: number } |
 * { type: 'min' } |
 * { type: 'mish' } |
 * { type: 'mpelu', alpha?: number, beta?: number } |
 * { type: 'mse' } |
 * { type: 'mtlu', a?: number | number[], b?: number | number[], c?: number | number[], k?: number } |
 * { type: 'mult' } |
 * { type: 'negative' } |
 * { type: 'nlrelu', beta?: number } |
 * { type: 'not' } |
 * { type: 'onehot', class_size?: number, values?: number[] } |
 * { type: 'or' } |
 * { type: 'output' } |
 * { type: 'pau', m?: number, n?: number, a?: number | number[], b?: number | number[] } |
 * { type: 'pdelu', t?: number, alpha?: number } |
 * { type: 'pelu', a?: number, b?: number } |
 * { type: 'plu', alpha?: number, c?: number } |
 * { type: 'power', n: number | string } |
 * { type: 'prelu', a?: number | number[] | string } |
 * { type: 'preu', alpha?: number, beta?: number } |
 * { type: 'psf', m?: number } |
 * { type: 'ptanh', a?: number } |
 * { type: 'ptelu', alpha?: number, beta?: number } |
 * { type: 'random', size: number | number[] | string, mean?: number, variance?: number } |
 * { type: 'relu' } |
 * { type: 'repu', s?: number } |
 * { type: 'resech' } |
 * { type: 'reshape', size: number[] | string } |
 * { type: 'reu' } |
 * { type: 'reverse', axis?: number } |
 * { type: 'rnn', size: number, out_size?: number, activation?: string, recurrent_activation?: string, return_sequences?: boolean, w_xh?: number[][] | Matrix, w_hh?: number[][] | Matrix, w_hy?: number[][] | Matrix, b_xh?: number[][] | Matrix, b_hh?: number[][] | Matrix, b_hy?: number[][] | Matrix, activation_params?: object, recurrent_activation_params?: object } |
 * { type: 'rootsig' } |
 * { type: 'round' } |
 * { type: 'rrelu', l?: number, u?: number } |
 * { type: 'rtrelu' } |
 * { type: 'selu', a?: number, g?: number } |
 * { type: 'sigmoid', a?: number } |
 * { type: 'sign' } |
 * { type: 'silu' } |
 * { type: 'sin' } |
 * { type: 'sinh' } |
 * { type: 'slaf', n?: number, a?: number | number[] } |
 * { type: 'slu', alpha?: number, beta?: number, gamma?: number } |
 * { type: 'soft_shrink', l?: number } |
 * { type: 'softargmax', beta?: number } |
 * { type: 'softmax' } |
 * { type: 'softplus', beta?: number } |
 * { type: 'softsign' } |
 * { type: 'sparsity', rho: number, beta: number } |
 * { type: 'split', axis?: number, size: number | number[] } |
 * { type: 'sqrt' } |
 * { type: 'square' } |
 * { type: 'srelu', d?: number } |
 * { type: 'srs', alpha?: number, beta?: number } |
 * { type: 'ssigmoid' } |
 * { type: 'stanh', a?: number, b?: number } |
 * { type: 'std', axis?: number } |
 * { type: 'sub' } |
 * { type: 'sum', axis?: number } |
 * { type: 'supervisor' } |
 * { type: 'swish', beta?: number } |
 * { type: 'taf', a?: number, b?: number } |
 * { type: 'tan' } |
 * { type: 'tanh' } |
 * { type: 'tanhexp' } |
 * { type: 'tanhshrink' } |
 * { type: 'thresholded_relu', a?: number } |
 * { type: 'transpose', axis: number[] } |
 * { type: 'variable', size: number[] | string, l2_decay?: number, l1_decay?: number, value?: number[] | number[][] | Tensor } |
 * { type: 'variance', axis?: number } |
 * { type: 'xor' }
 * )} PlainLayerObject
 */
