import{NeuralnetworkException}from"../../neuralnetwork.js";import Layer from"./base.js";export default class LRNLayer extends Layer{constructor({alpha:t=1e-4,beta:e=.75,k:i=2,n:s,channel_dim:a=-1,...n}){if(super(n),this._alpha=t,this._beta=e,this._k=i,this._n=s,this._channel_dim=a,-1!==this._channel_dim&&1!==this._channel_dim)throw new NeuralnetworkException("Invalid channel dimension.")}_index(t,e,i){return-1===this._channel_dim?[t,...i,e]:[t,e,...i]}calc(t){this._i=t;const e=-1===this._channel_dim?t.sizes[t.dimension-1]:t.sizes[1],i=[Math.floor((this._n-1)/2),Math.ceil((this._n-1)/2)];this._s=t.copy();const s=-1===this._channel_dim?1:2;for(let a=0;a<t.sizes[0];a++){const n=Array(t.dimension-2).fill(0);do{for(let s=0;s<e;s++){let h=0;for(let r=Math.max(0,s-i[0]);r<Math.min(e,s+i[1]+1);r++)h+=t.at(this._index(a,r,n))**2;this._s.set(this._index(a,s,n),this._k+this._alpha*h)}for(let e=0;e<n.length&&(n[e]++,!(n[e]<t.sizes[e+s]));e++)n[e]=0}while(n.some((t=>t>0)))}const a=this._s.copy();return a.broadcastOperate(t,((t,e)=>e/t**this._beta)),a}grad(t){const e=this._i.copy();return e.broadcastOperate(this._s,((t,e)=>e**-this._beta-2*this._beta*t**2*e**(-this._beta-1)*this._alpha)),e.broadcastOperate(t,((t,e)=>t*e)),e}toObject(){return{type:"lrn",alpha:this._alpha,beta:this._beta,k:this._k,n:this._n,channel_dim:this._channel_dim}}}LRNLayer.registLayer("lrn");